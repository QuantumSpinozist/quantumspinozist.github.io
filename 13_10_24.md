# 07.10.24 - 13.10.24: Differential Transformer

**Paper**: [https://arxiv.org/pdf/2410.05258](https://arxiv.org/pdf/2410.05258)

## Introduction

It is well known that (standard) transformers can often allocate a lot of attention to unimportant information.
Inspired by techniques from signal processing the authors introduce differential attention, in which the regular attention
output is instead calculated as the difference between two independent attention maps. The idea is that, similar to noise cancellation
in headphones, this cancels out noise in the attention pattern, ensuring attention is primarily allocated to relevant context.

The authors perform experiments which seem to suggest that this mechanism makes the resulting transformer more ressource efficient (both in terms of data and model size)
and boosts performance on tasks like (among others) long context processing, in context learning and key information retrieval.

## Method
Given an input $$ X \in  \mathbb{R}^{N \times d_{\text{model}}}$$ and the learnable projection matrices $$ W^Q, W^K, W^V \in  \mathbb{R}^{d_{\text{model}} \times 2d}$$
differential attention is defined as

$$ [Q_1; Q_2] = XW^Q, \quad [K_1; K_2] = XW^K, \quad V = XW^V $$

$$ \text{DiffAttn}(X) = \left( \text{softmax}\left( \frac{Q_1 K_1^T}{\sqrt{d}} \right) - \lambda \, \text{softmax}\left( \frac{Q_2 K_2^T}{\sqrt{d}} \right) \right) V.$$

The scalar $$ \lambda$$ is learnable. We reparameterize it as 

$$ \lambda = \exp(\lambda_{q_1} \cdot \lambda_{k_1}) - \exp(\lambda_{q_2} \cdot \lambda_{k_2}) + \lambda_{\text{init}}$$

with learned vectors $$ \lambda_{q_1}, \lambda_{k_1}, \lambda_{q_2}, \lambda_{k_2} \in \mathbb{R}^{d}$$ and a fixed initilization scalar as a bias (the authors go into detail about different initializations).

Getting multi-head differential attention from this is now pretty straight forward

$$\text{head}_i = \text{DiffAttn}(X; W_i^Q, W_i^K, W_i^V, \lambda) $$

$$\bar{\text{head}}_i = (1 - \lambda_{\text{init}}) \cdot \text{LN}(\text{head}_i) $$

$$\text{MultiHead}(X) = \text{Concat}(\bar{\text{head}}_1, \cdots, \bar{\text{head}}_h) W^O.$$

One subtlety is the inclusion of the $$ (1 - \lambda_{\text{init}}) $$ factor after the layer normalization. This factor ensures that the overall gradient flow remains similar to that
of a transformer, so hyperparameter dependency and stability during training remain comparable. Also note that the layer normalization is done via RMSNorm (also in future uses).

The overall architecture of the differential transformer now consists of $$L$$ layers with each layer consisting of one multi-head differential attention module followed by an FFN (in the form of SwiGLU)

$$Y^l = \text{MultiHead}(\text{LN}(X^l)) + X^l$$ 

$$X^{l+1} = \text{SwiGLU}(\text{LN}(Y^l)) + Y^l.$$


## Experiments

The authors compare the differential transformer to the regular one on six different experiments including:
language model evaluation using a variety of downstream tasks, model size scaling behaviour, performance on long sequences,
key information retrieval, contextual hallucination and in context learning.

They start by training a 3B-parameter diff transformer on 1T tokens and compare it to a transformer of similar size.
The diff transformer outperforms consistently on all tasks.

Next they train diff and regular transformers at different model sizes and numbers of tokens to compare the scaling behaviour.
Both models exhibit the characteristic scaling behaviour, but in both cases the diff transformer achieves the same performance with
only about 60% of the parameters or tokens.

For the next experiment they look at the NLL at different context length. We see that both models perform better as more context is added but
the differential transformer consistently lies below the regular one, meaning it is able to utilize the context more effectively.

To evaluate the key information retrieval capabilities, the authors use the Needle-In-A-Haystack test, where the model is tasked to retrieve a piece of information
from a very large context. Here the diff transformer significantly outperforms again, especially for more difficult tasks, where the regular transformers performance falls off a lot.

The model also outperforms on (many shot) in context learning, while also showing lower variance under permutations of the given examples (i.e. more robust). 

To test the amount of contextual hallucinations, both models are evaluated on different question answering and summarization tasks, diff outperforms again.

The authors also performed an analysis of activation outliers and some ablation studies which we omit.

## Conclusion

The core idea of the technique is simple and plausible. In conjunction with the good results of the experiments it certainly seems interesting, especially
because the observed effects mostly fit quite neatly into the noise cancellation interpretation. I would have liked to see some experiments comparing the method to other attention variants
like different versions of sparse attention. Sparse attention methods promise many of the same upsides and have already seen use in very large models like GPT-4, so it would have been good
to explore how these older methods differ from this new one.

Ultimately time will tell if these results pan out to be useful in larger models.
