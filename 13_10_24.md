# 07.10.24 - 13.10.24: Differential Transformer

**Paper**: [https://arxiv.org/pdf/2410.05258](https://arxiv.org/pdf/2410.05258)

## Introduction


## Method
Given an input $$ X \in  \mathbb{R}^{N \times d_{\text{model}}}$$ and the learnable projection matrices $$ W^Q, W^K, W^V \in  \mathbb{R}^{d_{\text{model}} \times 2d}$$
differential attention is defined as

$$ [Q_1; Q_2] = XW^Q, \quad [K_1; K_2] = XW^K, \quad V = XW^V $$

$$ \text{DiffAttn}(X) = \left( \text{softmax}\left( \frac{Q_1 K_1^T}{\sqrt{d}} \right) - \lambda \, \text{softmax}\left( \frac{Q_2 K_2^T}{\sqrt{d}} \right) \right) V.$$

The scalar $$ \lambda$$ is learnable. We reparameterize it as 

$$ \lambda = \exp(\lambda_{q_1} \cdot \lambda_{k_1}) - \exp(\lambda_{q_2} \cdot \lambda_{k_2}) + \lambda_{\text{init}}$$

with learned vectors $$ \lambda_{q_1}, \lambda_{k_1}, \lambda_{q_2}, \lambda_{k_2} \in \mathbb{R}^{d}$$ and a fixed initilization scalar as a bias (the authors go into detail about different initializations).


## Experiments


