# 07.10.24 - 13.10.24: Differential Transformer

**Paper**: [https://arxiv.org/pdf/2410.05258](https://arxiv.org/pdf/2410.05258)

## Introduction


## Method
Given an input $$ X \in  \mathbb{R}^{N \times d_{\text{model}}}$$ and the learnable projection matrices $$ W^Q, W^K, W^V \in  \mathbb{R}^{d_{\text{model}} \times 2d}$$
differential attention is defined as

$$ [Q_1; Q_2] = XW^Q, \quad [K_1; K_2] = XW^K, \quad V = XW^V $$

$$ \text{DiffAttn}(X) = \left( \text{softmax}\left( \frac{Q_1 K_1^T}{\sqrt{d}} \right) - \lambda \, \text{softmax}\left( \frac{Q_2 K_2^T}{\sqrt{d}} \right) \right) V.$$

The scalar $$ \lambda$$ is learnable. We reparameterize it as 

$$ \lambda = \exp(\lambda_{q_1} \cdot \lambda_{k_1}) - \exp(\lambda_{q_2} \cdot \lambda_{k_2}) + \lambda_{\text{init}}$$

with learned vectors $$ \lambda_{q_1}, \lambda_{k_1}, \lambda_{q_2}, \lambda_{k_2} \in \mathbb{R}^{d}$$ and a fixed initilization scalar as a bias (the authors go into detail about different initializations).

Getting multi-head differential attention from this is now pretty straight forward

$$\text{head}_i = \text{DiffAttn}(X; W_i^Q, W_i^K, W_i^V, \lambda) $$

$$\bar{\text{head}}_i = (1 - \lambda_{\text{init}}) \cdot \text{LN}(\text{head}_i) $$

$$\text{MultiHead}(X) = \text{Concat}(\bar{\text{head}}_1, \cdots, \bar{\text{head}}_h) W^O.$$

One subtlety is the inclusion of the $$ (1 - \lambda_{\text{init}}) $$ factor after the layer normalization. This factor ensures that the overall gradient flow remains similar to that
of a transformer, so hyperparameter dependency and stability during training remain comparable. Also note that the layer normalization is done via RMSNorm (also in future uses).

The overall architecture of the differential transformer now consists of $$L$$ layers with each layer consisting of one multi-head differential attention module followed by an FFN (in the form of SwiGLU)

$$Y^l = \text{MultiHead}(\text{LN}(X^l)) + X^l$$ 

$$X^{l+1} = \text{SwiGLU}(\text{LN}(Y^l)) + Y^l.$$



## Experiments


