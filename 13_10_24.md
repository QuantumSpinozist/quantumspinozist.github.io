# 07.10.24 - 13.10.24: Differential Transformer

**Paper**: [https://arxiv.org/pdf/2410.05258](https://arxiv.org/pdf/2410.05258)

## Introduction


## Method
Given an input $$ X \in  \mathbb{R}^{N \times d_{\text{model}}}$$ and the learnable projection matrices $$ W^Q, W^K, W^V \in  \mathbb{R}^{d_{\text{model}} \times 2d}$$
differential attention is defined as

$$ [Q_1; Q_2] = XW^Q, \quad [K_1; K_2] = XW^K, \quad V = XW^V $$

$$ \text{DiffAttn}(X) = \left( \text{softmax}\left( \frac{Q_1 K_1^T}{\sqrt{d}} \right) - \lambda \, \text{softmax}\left( \frac{Q_2 K_2^T}{\sqrt{d}} \right) \right) V.$$

The scalar $$ \lambda$$ is learnable. We reparameterize it as 

$$ \lambda = \exp(\lambda_{q_1} \cdot \lambda_{k_1}) - \exp(\lambda_{q_2} \cdot \lambda_{k_2}) + \lambda_{\text{init}}$$

with learned vectors $$ \lambda_{q_1}, \lambda_{k_1}, \lambda_{q_2}, \lambda_{k_2} \in \mathbb{R}^{d}$$ and a fixed initilization scalar as a bias (the authors go into detail about different initializations).

Getting multi-head differential attention from this is now pretty straight forward

$$\text{head}_i = \text{DiffAttn}(X; W_i^Q, W_i^K, W_i^V, \lambda) $$

$$\bar{\text{head}}_i = (1 - \lambda_{\text{init}}) \cdot \text{LN}(\text{head}_i) $$

$$\text{MultiHead}(X) = \text{Concat}(\bar{\text{head}}_1, \cdots, \bar{\text{head}}_h) W^O.$$

One subtlety is the inclusion of the $$ (1 - \lambda_{\text{init}}) $$ factor after the layer normalization. This factor ensures that the overall gradient flow remains similar to that
of a transformer, so hyperparameter dependency and stability during training remain comparable. Also note that the layer normalization is done via RMSNorm (also in future uses).

The overall architecture of the differential transformer now consists of $$L$$ layers with each layer consisting of one multi-head differential attention module followed by an FFN (in the form of SwiGLU)

$$Y^l = \text{MultiHead}(\text{LN}(X^l)) + X^l$$ 

$$X^{l+1} = \text{SwiGLU}(\text{LN}(Y^l)) + Y^l.$$


## Experiments

The authors compare the differential transformer to the regular one on six different experiments including:
language model evaluation using a variety of downstream tasks, model size scaling behaviour, performance on long sequences,
key information retrieval, contextual hallucination and in context learning.

They start by training a 3B-parameter diff transformer on 1T tokens and compare it to a transformer of similar size.
The diff transformer outperforms consistently on all tasks.

Next they train diff and regular transformers at different model sizes and numbers of tokens to compare the scaling behaviour.
Both models exhibit the characteristic scaling behaviour, but in both cases the diff transformer achieves the same performance with
only about 60% of the parameters or tokens.

For the next experiment they look at the NLL at different context length. We see that both models perform better as more context is added but
the differential transformer consistently lies below the regular one, meaning it is able to utilize the context more effectively.

To evaluate the key information retrieval capabilities, the authors use the Needle-In-A-Haystack test, where the model is tasked to retrieve a piece of information
from a very large context. Here the diff transformer significantly outperforms again, especially for more difficult tasks, where the regular transformers performance falls off
a lot.
