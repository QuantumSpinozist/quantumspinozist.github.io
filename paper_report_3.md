# Paper Report 3: Auto-Encoding Variational Bayes

**Paper**: [https://arxiv.org/pdf/1312.6114](https://arxiv.org/pdf/1312.6114)

## Introduction

## Problem statement

We are given and i.i.d. dataset $$ \{ x^{i} \}_{i=0, ..., N}$$ that where $$x$$ is generated by a latent variable $$z$$ in dependence of the parameter vector $$\theta $$ via

$$z \sim p_{\theta^*}(z),\\ x|z \sim p_{\theta^*}(x|z).$$

Both PDFs are assumed to be differentiable in $$z$$ and $$\theta$$ almost everywhere. Now the central idea of variational bayes methods is to approximate the true
a posteriori distribution $$ p_{\theta^*}(z|x) $$ by some parametric distribution $$ q_{\phi}(z|x)$$ for the optimal choice of $$ \phi $$.
Ultimately we will want to represent both $$ p_{\theta^*}(z|x) $$ and $$ q_{\phi}(z|x) $$ as MLPs. In the language of autoencoders $$ p_{\theta^*}(z|x) $$
is therefore called the encoder and $$ q_{\phi}(z|x) $$ the decoder.



## The Evidence Lower Bound (ELBO)

In the paper the name variational lower bound is used, but evidence lower bound seems to be more common nowadays (at least in my experience).
The marginal likelihood of each datapoint can be expressed as 

$$ \log(p_{\theta}(x)) = D_{KL}(q_{\phi}(z|x) \mid\mid p_{\theta}(z|x) ) + L(\theta, \phi; x)$$

where $$D_{KL}( \cdot \mid\mid \cdot )$$ is the Kullbackâ€“Leibler divergence and $$L(\theta, \phi; x)$$ is the aforementioned evidence lower bound which can
be expressed as

$$ L(\theta, \phi; x) =  -D_{KL}(q_{\phi}(z|x) \mid\mid p_{\theta}(z) ) + \mathbb{E}_{q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right].$$

Since the KL divergence is strictly non-negative, $$ L(\theta, \phi; x) $$ can be used as a lower bound on the marginal log-likelihood (as the name suggests).
Therefore maximizing ELBO (or minimizing its negative) defines our learning objective. But naively applying gradient based optimization algorithms would lead to problems
now. This is because the regular Monte Carlo estimatior of the gradient in $$ \phi $$ has an impractically high variance making the approximation effectively unusable.

## The reparameterization trick, the SGVB estimator and the AEVB algorithm

In order to circumvent this problem the authors propose the so called reparameterization trick. Therein we express
the latent variable $$z \sim q_{\phi}(z|x)$$ via an auxiliary and static (as in fully unparameterized) random variable
$$ \epsilon \sim  p(\epsilon)$$ as $$ z = g_{\phi}(\epsilon, x) $$. To be concrete, in the context of the standard variational encoder
$$ z|x$$ will be assumed to be gaussian with mean and standard deviation being produced by the encoder network. The reparameterization function
is then simply $$ g_{\phi}(\epsilon, x) = \mu + \sigma \cdot \epsilon $$ with $$ \epsilon \sim \mathcal{N}(0, 1)$$.

The paper presents to different ways to approximate the evidence lower bound $L(\theta, \phi; x)$ (essentially the loss function), also called Stochastic Gradient Variational Bayes
(SGVB) estimators. From the form $L(\theta, \phi; x) = \mathbb{E}_{q_{\phi}(z|x)} \left[ \log p_{\theta}(x, z) - \log q_{\phi}(z | x) \right]$ you get the generic SGVB estimator by applying
the simple MC approximation where we take $l$ samples of $\epsilon$ and calculate $z$ using the reparameterization trick

$$ L^A(\theta, \phi; x) =\frac{1}{l} \sum_{k = 1}^L( \log p_{\theta}(x, z^{(k)}) - \log q_{\phi}(z^{(k)} | x) ) \approx L(\theta, \phi; x).$$

In many cases it is possible to anaytically determine the KL-Divergence so one can use the other form of the ELBO we presented earlier. The resulting
estimator usually has a smaller variance

$$ L^B(\theta, \phi; x) =  -D_{KL}(q_{\phi}(z|x) \mid\mid p_{\theta}(z) ) + \frac{1}{l} \sum_{k = 1}^L ( \log p_{\theta}(x|z^{(k)}) ).$$




