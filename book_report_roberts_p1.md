# Book Report: The Principles of Deep Learning Theory - Part 1, Neural Networks at Initialization

**Book** [https://arxiv.org/pdf/2106.10165](https://arxiv.org/pdf/2106.10165)

## -1. Introduction to this Report
This report aims to summarize the main takeaways of the book in a more or less easily consumable
manor. It does not in any way substitute the original text, especially since I will radically skip the majority of the technical derivations, 
which one might rightfully argue really form the meat of the book (in particular to someone who wishes to adapt some of the techniques for
their own theoretical undertakings). I highly suggest reading the book, even if the long messy calculations
can be somewhat hard to get through, especially for people who are unfamiliar with field theory. Under this approach
this report should be seen as a mostly supplementary ressource, designed to help you see both forest and the trees.
Nevertheless I hope this text can also be appreciated by people who do not intend to read the whole book.

Unlike in my usual reports I have stuck with the structure of the book in terms of chapter numbering, but I have slightly adjusted the chapter titles at times.
Part 1 will cover the first half of the book which studies MLPs at weight initialization.

## 0. Introduction

## 1. Preparation

## 2. Neural Networks

## 3. Effective Theory of Deep Linear Networks at Initialization

## 4. RG Flow of Preactivations

## 5. Effective Theory of Preactivations at Initialization
