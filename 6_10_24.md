# 30.09.24 - 06.10.24: Were RNNs All We Needed?

**Paper**: [https://arxiv.org/abs/2410.01201](https://arxiv.org/abs/2410.01201)

## Introduction

Even though transformers are the central technique for sequence modelling today, due
to dot product attention having quadratic complexity in the sequence length, they struggle
with long sequences. There have been numerous efforts to develop model types that are on par
with transformers but circumvent this issue. Namely in recent years state-space models like Mamba and S4,
as well as attention based models like Aaren have been proposed in this respect. Many of these models have
reached SOTA performance on squenece modelling tasks.

All these approaches have in common that they use the parallel prefix scan algorithm (PPS) to reduce computation time.
The overacrhing question the authors ask now is very simple: what is the minimal model based on PPS that can still compete with
these modern approaches? Concretely they develop minimal versions of LSTM and GRU that can utilize PPS, thereby eliviating the main problem
of these older models, namely the high cost of backpropogation through time (BPTT).

## Method

We will largely focus on GRUs, as it is really sufficient to understand the overall idea. Unlike the paper we will also not revise GRUs extensively but just
state the central reccurence relations

$$ z_t = \sigma(\text{Linear}_z([x_t, h_{t-1}])) $$

$$ r_t = \sigma(\text{Linear}_r([x_t, h_{t-1}])) $$

$$ \tilde{h}_t = \tanh(\text{Linear}_h([x_t, r_t \circ h_{t-1}])) $$

$$ h_t = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t. $$

The PPS algorithm can be applied to functions of the form $$ v_t = a_t v_{tâˆ’1} + b_t $$ which does resemble $$ h_t = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t .$$
But as the formulae show both $$z_t$$ and $$ \tilde{h}_t $$ do depend on $$h_{t-1}$$ implicitly. If we get rid of all additional dependcies of the previous state besides the one in the stated
equation the formulae reduce to
