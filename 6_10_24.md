# 30.09.24 - 06.10.24: Were RNNs All We Needed?

**Paper**: [https://arxiv.org/abs/2410.01201](https://arxiv.org/abs/2410.01201)

## Introduction

Even though transformers are the central technique for sequence modelling today, due
to dot product attention having quadratic complexity in the sequence length, they struggle
with long sequences. There have been numerous efforts to develop model types that are on par
with transformers but circumvent this issue. Namely in recent years state-space models like Mamba and S4,
as well as attention based models like Aaren have been proposed in this respect. Many of these models have
reached SOTA performance on squenece modelling tasks.

All these approaches have in common that they use the parallel prefix scan algorithm (PPS) to reduce computation time.
The overacrhing question the authors ask now is very simple: what is the minimal model based on PPS that can still compete with
these modern approaches? Concretely they develop minimal versions of LSTM and GRU (called minLSTM and minGRU ) that can utilize PPS, thereby eliviating the main problem
of these older models, namely the high cost of backpropogation through time (BPTT).

## Method

We will largely focus on minGRUs, as it is really sufficient to understand the overall idea. Unlike the paper we will also not revise GRUs extensively but just
state the central reccurence relations

$$ z_t = \sigma(\text{Linear}_z([x_t, h_{t-1}])) $$

$$ r_t = \sigma(\text{Linear}_r([x_t, h_{t-1}])) $$

$$ \tilde{h}_t = \tanh(\text{Linear}_h([x_t, r_t \circ h_{t-1}])) $$

$$ h_t = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t. $$

The PPS algorithm can be applied to functions of the form $$ v_t = a_t v_{tâˆ’1} + b_t $$ which does resemble $$ h_t = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t .$$
But as the formulae show both $$z_t$$ and $$ \tilde{h}_t $$ do depend on $$h_{t-1}$$ implicitly. If we get rid of all additional dependcies of the previous state besides the one in the stated
equation the formulae reduce to
    
$$z_t = \sigma(\text{Linear}_{d_h}(x_t)) $$
    
$$\tilde{h}_t = \text{Linear}_{d_h}(x_t). $$

$$ h_t = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t .$$

Notice that $$r_t$$ can be removed completely. Also notice that we have removed the $$\tanh$$ from the hidden state proposition. Before, the hidden states were passed through a sigmoid
in the calculations of $$z_t$$ and $$ r_t $$, so they needed to be normalized to avoid vanishing gradients. This is no longer the case, so one can drop the $$\tanh$$.

In this minimal form BPTT is no longer necessary, the models can be efficiently trained using PPS. In particular this reduces the dependence on the sequence length from linear to constant time.

The minimal models as is are seemingly weaker than the original LSTM and GRU since we removed the dependence of the hidden state proposal on the previous state.
Or said another way, the dependence of the current hidden state on the last hidden state is much less flexible. This issue gets partially weakened as one adds more layers, because
the input to the second layer (and therefore also the state proposal) depends on the previous hidden state of the first layer.

## Experiments

First the authors compare the training times of LSTM, GRU, minLSTM, minGRU and Mamba proving that the complexity indeed becomes complex for the minimal models and is equivalent to that
of Mamba. At the same time the memory of the minimal models increases, sitting between Mamba (which is even higher) and the original models.

In terms performance they look at three tasks: the Selective Copying task from the original Mamba paper, some RL tasks and a very small language modelling experiment based on the shapespeare dataset.


